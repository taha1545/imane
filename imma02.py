from flask import Flask, render_template, request, jsonify # pyright: ignore[reportMissingImports]
import uuid
import datetime
import re
import logging
import unicodedata
import os
import json
import difflib
import random
import shutil
import subprocess
import numpy as np # pyright: ignore[reportMissingImports]

# --- 1. Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø´Ø±Ø·ÙŠ Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙˆØ§Ù„ØµÙˆØª ---
# (ÙŠØ³Ù…Ø­ Ø¨ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø­ØªÙ‰ Ù„Ùˆ Ù„Ù… ØªÙƒÙ† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø© Ù…Ø«Ø¨ØªØ©)

librosa = None
sf = None
try:
    import librosa # pyright: ignore[reportMissingImports]
    import soundfile as sf # pyright: ignore[reportMissingImports]
except ImportError:
    pass

TORCH_AVAILABLE = False
torch = None
try:
    import importlib
    # Only import torch dynamically if it's installed to avoid static import errors in editors/linters
    if importlib.util.find_spec("torch") is not None:
        torch = importlib.import_module("torch")
        TORCH_AVAILABLE = True
except Exception:
    torch = None
    TORCH_AVAILABLE = False

AutoModelForSequenceClassification = None
AutoTokenizer = None
try:
    import importlib
    if importlib.util.find_spec("transformers") is not None and TORCH_AVAILABLE:
        mod = importlib.import_module("transformers")
        AutoModelForSequenceClassification = getattr(mod, "AutoModelForSequenceClassification", None)
        AutoTokenizer = getattr(mod, "AutoTokenizer", None)
except ImportError:
    pass

# ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¬Ù‡Ø§Ø² (Ø¨Ø´ÙƒÙ„ Ø¢Ù…Ù† Ø¥Ù† Ù„Ù… ÙŠØªÙˆÙØ± torch)
if TORCH_AVAILABLE and torch is not None:
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
else:
    device = "cpu"

# --- 2. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙˆØ§Ù„ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ---

app = Flask(__name__)
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- 3. Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙ†ØµÙˆØµ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ---

# ... (EMOTION_RESPONSES, EMOTION_POLARITY, _ARABIC_PREFIXES, _ARABIC_SUFFIXES, _ARABIC_NEGATION, CRISIS_KEYWORDS, GREETING_RESPONSES, SPECIAL_RESPONSES, FUNNY_RESPONSES, CALM_SUPPORT_PHRASES, CRISIS_RESPONSE_AR, CRISIS_RESPONSE_EN, TOPIC_PROMPTS, ALGERIA_RESOURCES, ISLAMIC_PHRASES, QURAN_VERSES, APP_DISCLAIMER) ...
# (Ù…Ù„Ø§Ø­Ø¸Ø©: ØªÙ… Ø­Ø°Ù Ù‡Ø°Ù‡ Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„Ù†ØµÙŠØ© Ø§Ù„Ø·ÙˆÙŠÙ„Ø© Ù„Ù„Ø§Ø®ØªØµØ§Ø± Ù‡Ù†Ø§ ÙˆØ¥Ø¨Ù‚Ø§Ø¦Ù‡Ø§ ÙƒÙ…Ø§ Ù‡ÙŠ ÙÙŠ ÙƒÙˆØ¯Ùƒ)

EMOTION_RESPONSES = {
    "Ø³Ø¹ÙŠØ¯": "ğŸ˜Š Ø³Ø¹ÙŠØ¯ Ù„Ø³Ù…Ø§Ø¹ Ø°Ù„Ùƒ! Ù‡Ù„ ØªÙˆØ¯ Ù…Ø´Ø§Ø±ÙƒØ© Ù…Ø§ ÙŠØ¬Ø¹Ù„Ùƒ Ø³Ø¹ÙŠØ¯Ø§Ù‹ØŸ",
    "Ø­Ø²ÙŠÙ†": "ğŸ˜¢ Ø£Ø±Ù‰ Ø£Ù†Ùƒ Ø­Ø²ÙŠÙ†ØŒ Ø£Ù†Ø§ Ù‡Ù†Ø§ Ù„Ù„Ø§Ø³ØªÙ…Ø§Ø¹. Ù‡Ù„ ØªØ±ÙŠØ¯ Ø£Ù† ØªØ®Ø¨Ø±Ù†ÙŠ Ø¨Ù…Ø§ ÙŠØ­Ø¯Ø«ØŸ",
    "ØºØ§Ø¶Ø¨": "ğŸ˜  ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„ØºØ¶Ø¨ Ù…Ø±Ù‡Ù‚Ø§Ù‹. Ø®Ø° Ù†ÙØ³Ø§Ù‹ Ø¹Ù…ÙŠÙ‚Ø§Ù‹ØŒ Ù‡Ù„ ØªÙˆØ¯ Ø£Ù† ØªØªØ­Ø¯Ø« Ø¹Ù† Ø§Ù„Ø³Ø¨Ø¨ØŸ",
    "ÙˆØ­Ø¯Ø©": "ğŸ’› Ø£Ø´Ø¹Ø± Ø¨Ùƒ. Ø§Ù„ÙˆØ­Ø¯Ø© ØµØ¹Ø¨Ø© Ù„Ù„ØºØ§ÙŠØ© â€” Ù‡Ù„ ØªÙˆØ¯ ØªØ¬Ø±Ø¨Ø© Ø¨Ø¹Ø¶ Ø§Ù„ØªÙ…Ø§Ø±ÙŠÙ† Ø§Ù„Ø³Ø±ÙŠØ¹Ø© Ù„Ù„ØªØ®ÙÙŠÙ Ø£Ùˆ Ù…Ø´Ø§Ø±ÙƒØ© Ù…Ø§ ØªØ´Ø¹Ø± Ø¨Ù‡ Ø§Ù„Ø¢Ù†ØŸ",
    "Ø®Ø§Ø¦Ù": "ğŸ˜¨ Ø§Ù„Ø®ÙˆÙ Ø´Ø¹ÙˆØ± Ø·Ø¨ÙŠØ¹ÙŠ. Ù‡Ù„ ØªØ±ØºØ¨ Ø£Ù† Ù†Ø®Ø·Ø· Ù…Ø¹Ø§Ù‹ Ø®Ø·ÙˆØ§Øª ØµØºÙŠØ±Ø© Ù„Ù„Ø´Ø¹ÙˆØ± Ø¨Ø§Ù„Ø£Ù…Ø§Ù†ØŸ",
    "Ù‚Ù„Ù‚": "ğŸ˜Ÿ Ø§Ù„Ù‚Ù„Ù‚ Ù…Ø²Ø¹Ø¬ØŒ Ø¯Ø¹Ù†Ø§ Ù†Ø¨Ø³Ø· Ø§Ù„Ø£Ù…ÙˆØ± ÙˆÙ†Ø¨Ø¯Ø£ Ø¨ØªÙ†ÙØ³ Ù‡Ø§Ø¯Ø¦ Ù…Ø¹ Ø¨Ø¹Ø¶ Ø§Ù„Ø£Ø³Ø§Ù„ÙŠØ¨ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©.",
}
CRISIS_KEYWORDS = [
    "Ø§Ù†ØªØ­Ø±", "Ø£Ø±ÙŠØ¯ Ø£Ù† Ø£Ù†ØªØ­Ø±", "Ø£ÙÙ†Ù‡ÙÙŠ Ø­ÙŠØ§ØªÙŠ", "Ù‚ØªÙ„ Ù†ÙØ³ÙŠ", "suicide", "end my life"
]
APP_DISCLAIMER = (
     "Ù…Ù„Ø§Ø­Ø¸Ø©: Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ù„ÙŠØ³ Ø¨Ø¯ÙŠÙ„Ø§Ù‹ Ø¹Ù† Ø§Ù„Ø¹Ù„Ø§Ø¬ Ø§Ù„Ù†ÙØ³ÙŠ Ø§Ù„Ù…Ø­ØªØ±Ù. Ø¥Ø°Ø§ ÙƒÙ†Øª ØªÙÙƒØ± ÙÙŠ Ø¥ÙŠØ°Ø§Ø¡ Ù†ÙØ³ÙƒØŒ "
     "Ø§ØªØµÙ„ Ø¨Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø·Ø§Ø±Ø¦Ø© Ø£Ùˆ Ø¨Ø®Ø· Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„Ù†ÙØ³ÙŠ Ø§Ù„Ù…Ø­Ù„ÙŠ ÙÙˆØ±Ø§Ù‹. ÙŠÙ…ÙƒÙ†Ù†ÙŠ ØªÙ‚Ø¯ÙŠÙ… Ø¯Ø¹Ù… Ù„Ø­Ø¸ÙŠ ÙˆØ¥Ø¬Ø±Ø§Ø¡Ø§Øª ØªÙ‡Ø¯Ø¦Ø© Ù‚ØµÙŠØ±Ø©."
)
GREETING_RESPONSES = {
    "Ø³Ù„Ø§Ù…": "ğŸ‘‹ ÙˆØ¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù…! ÙƒÙŠÙ Ø£Ù‚Ø¯Ø± Ø£Ø³Ø§Ø¹Ø¯Ùƒ Ø§Ù„ÙŠÙˆÙ…ØŸ",
    "Ù…Ø±Ø­Ø¨Ø§": "ğŸ‘‹ Ù…Ø±Ø­Ø¨Ø§Ù‹! Ø£Ù†Ø§ ImmaØŒ Ù‡Ù†Ø§ Ù„Ù„Ø§Ø³ØªÙ…Ø§Ø¹ Ø¹Ù†Ø¯Ù…Ø§ ØªØ­ØªØ§Ø¬.",
}
# ... (Ø¨Ù‚ÙŠØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª) ...

# ØªØ¹Ø±ÙŠÙØ§Øª Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ù„ØªØ¬Ù†Ø¨ Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø§Ø³Ù… Ø¹Ù†Ø¯ Ø¹Ø¯Ù… ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„ÙƒØ§Ù…Ù„Ø©
SPECIAL_RESPONSES = {}
EMOTION_POLARITY = {
    # Ø£Ù…Ø«Ù„Ø© Ù‚Ù„ÙŠÙ„Ø© Ù„ØªØ¬Ù†Ø¨ Ø£Ø®Ø·Ø§Ø¡ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø­Ø³Ø§Ø¨Ø› ÙŠÙØ³ØªØ¨Ø¯Ù„ Ø¨Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„ÙƒØ§Ù…Ù„ Ø¥Ù† ÙˆÙØ¬Ø¯
    "Ø³Ø¹ÙŠØ¯": 0.8, "Ù…Ø¨Ø³ÙˆØ·": 0.9, "ÙØ±Ø­Ø§Ù†": 1.0, "Ù…ØªÙØ§Ø¦Ù„": 0.7,
    "Ø­Ø²ÙŠÙ†": -0.8, "Ù…ÙƒØªØ¦Ø¨": -0.9, "Ù…ØªØ¶Ø§ÙŠÙ‚": -0.4, "Ù…Ø­Ø¨Ø·": -0.7, "ÙŠØ£Ø³": -1.0,
    "ØºØ§Ø¶Ø¨": -0.8, "Ø¹ØµØ¨ÙŠ": -0.7, "Ù…Ø³ØªÙØ²": -0.6,
    "Ù‚Ù„Ù‚": -0.6, "Ù…ØªÙˆØªØ±": -0.7, "Ø®Ø§ÙŠÙ": -0.5,
    "ÙˆØ­Ø¯Ø©": -0.8, "ÙˆØ­ÙŠØ¯": -0.8,
    "ØªØ¹Ø¨Ø§Ù†": -0.4, "Ù…Ø±Ù‡Ù‚": -0.5,
    # ÙƒÙ„Ù…Ø§Øª Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©
    "Ø±Ø§Ø¦Ø¹": 0.9, "Ù…Ù…ØªØ§Ø²": 1.0, "Ø¬Ù…ÙŠÙ„": 0.8, "Ù…Ø°Ù‡Ù„": 0.9,
    # ÙƒÙ„Ù…Ø§Øª Ø³Ù„Ø¨ÙŠØ©
    "Ø³ÙŠØ¡": -0.7, "ÙØ§Ø´Ù„": -0.9, "Ù…Ø´ÙƒÙ„Ø©": -0.5, "ØµØ¹Ø¨": -0.4,
}
CALM_SUPPORT_PHRASES = [
    "Ø®Ø° Ù†ÙØ³Ø§Ù‹ Ø¹Ù…ÙŠÙ‚Ø§Ù‹ ÙˆØ±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø­Ø³Ø§Ø³ Ø§Ù„Ø¢Ù†.",
    "Ø£Ù†Ø§ Ù‡Ù†Ø§ Ù…Ø¹Ùƒ. Ø­Ø§ÙˆÙ„ Ø£Ù† ØªØ°ÙƒØ± Ù…Ø§ ØªØ´Ø¹Ø± Ø¨Ù‡ Ø¨Ø§Ù„ØªØ­Ø¯ÙŠØ¯ØŒ Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©.",
    "Ø¬Ø±Ø¨ Ø¥Ø±Ø³Ø§Ù„ Ø±Ø³Ø§Ù„Ø© Ù‚ØµÙŠØ±Ø© Ù„ØµØ¯ÙŠÙ‚ Ø£Ùˆ Ø§Ù„Ø§Ù†Ø¶Ù…Ø§Ù… Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù‡ØªÙ…Ø© Ø¨Ù‡ÙˆØ§ÙŠØ© ØªØ­Ø¨Ù‡Ø§ â€” ØªÙˆØ§ØµÙ„ Ø¨Ø³ÙŠØ· Ù‚Ø¯ ÙŠØ®ÙÙ Ø§Ù„Ø´Ø¹ÙˆØ± Ø¨Ø§Ù„ÙˆØ­Ø¯Ø©.",
    "Ù‚Ø¯ ÙŠØ³Ø§Ø¹Ø¯ Ø§Ù„Ù…Ø´ÙŠ Ø§Ù„Ù‚ØµÙŠØ± ÙÙŠ Ø§Ù„Ù‡ÙˆØ§Ø¡ Ø§Ù„Ø·Ù„Ù‚ Ø£Ùˆ Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ Ù„Ù…ÙˆØ³ÙŠÙ‚Ù‰ Ù…Ø±ÙŠØ­Ø© Ø§Ù„Ø¢Ù†."
]
CRISIS_RESPONSE_AR = "Ø¥Ø°Ø§ ÙƒÙ†Øª ØªÙÙƒØ± ÙÙŠ Ø¥ÙŠØ°Ø§Ø¡ Ù†ÙØ³ÙƒØŒ Ø§ØªØµÙ„ Ø¨Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø·Ø§Ø±Ø¦Ø© Ø£Ùˆ Ø¨Ø®Ø· Ø§Ù„Ø¯Ø¹Ù… ÙÙˆØ±Ø§Ù‹."
# ----------------- Ø¯Ø§Ù„Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø§Øª Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ© -----------------

RESPONSES_PATH = os.path.join(os.path.dirname(__file__), "data", "responses.json")
EXAMPLE_PHRASES = []
PHRASE_BANK = {}
MOTIVATIONAL_QUOTES = [
    "ÙƒÙ„ ÙŠÙˆÙ… Ø®Ø·ÙˆØ© ØµØºÙŠØ±Ø© ØªÙ‚ÙˆØ¯Ùƒ Ø¥Ù„Ù‰ ØªØºÙŠÙŠØ± ÙƒØ¨ÙŠØ±.",
    "Ø£Ù†Øª Ù„Ø³Øª ÙˆØ­Ø¯Ùƒ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø·Ø±ÙŠÙ‚ â€” ØªØ­Ø¯Ø« Ø¹Ù†Ø¯Ù…Ø§ ØªØ­ØªØ§Ø¬.",
    "Ø§Ù„Ø§Ø¹ØªÙ†Ø§Ø¡ Ø¨Ù†ÙØ³Ùƒ Ø§Ù„ÙŠÙˆÙ… Ù‡Ùˆ Ø¬Ø³Ø± Ù„ØºØ¯Ù Ø£ÙØ¶Ù„.",
    "Ù„Ø§ Ø¨Ø£Ø³ Ø¨Ø£Ù† ØªØ¨Ø¯Ø£ Ø¨Ø­Ø§Ø¬Ø² ØµØºÙŠØ±Ø› Ø§Ù„Ù…Ù‡Ù… Ø£Ù† ØªØ¨Ø¯Ø£.",
    "Ø§Ù„ØªÙ‚Ø¯Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ ÙŠØ£ØªÙŠ Ù…Ù† Ø§Ù„ØªØ²Ø§Ù… ÙŠÙˆÙ…ÙŠ Ø¨Ø³ÙŠØ·."
]
YOUTH_RESOURCES = {}
SPECIAL_RESPONSES = {}
CRISIS_RESPONSE_AR = "Ø¥Ø°Ø§ ÙƒÙ†Øª ØªÙÙƒØ± ÙÙŠ Ø¥ÙŠØ°Ø§Ø¡ Ù†ÙØ³ÙƒØŒ Ø§ØªØµÙ„ Ø¨Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø·Ø§Ø±Ø¦Ø© Ø£Ùˆ Ø¨Ø®Ø· Ø§Ù„Ø¯Ø¹Ù… ÙÙˆØ±Ø§Ù‹."
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_external_responses(defaults):
    """
    ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†ØµÙŠØ© Ù…Ù† Ù…Ù„Ù JSON Ø®Ø§Ø±Ø¬ÙŠ ÙˆØ¯Ù…Ø¬Ù‡Ø§ Ù…Ø¹ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©.
    Ù‡Ø°Ø§ ÙŠÙ‚Ù„Ù„ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… `global` ÙˆÙŠØ¬Ø¹Ù„ ØªØ¯ÙÙ‚ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£ÙƒØ«Ø± ÙˆØ¶ÙˆØ­Ù‹Ø§.
    """
    try:
        with open(RESPONSES_PATH, "r", encoding="utf-8-sig") as fh:
            data = json.load(fh)
        
        # Ø¯Ù…Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù…Ù„Ø© Ù…Ø¹ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
        greetings = {**defaults['greetings'], **data.get("greetings", {})}
        emotions = {**defaults['emotions'], **data.get("emotions", {})}
        special = {**defaults['special'], **data.get("special", {})}
        examples = data.get("examples", defaults['examples'])
        phrase_bank = data.get("phrase_bank", defaults['phrase_bank'])
        quotes = data.get("motivational_quotes", defaults['quotes'])
        resources = data.get("youth_resources", defaults['resources'])
        
        logger.info("Loaded external responses from %s", RESPONSES_PATH)
        return greetings, emotions, special, examples, phrase_bank, quotes, resources

    except FileNotFoundError:
        logger.warning("Responses file not found at %s. Using default values.", RESPONSES_PATH)
        return defaults.values()
    except (json.JSONDecodeError, Exception):
        logger.exception("Failed to load external responses")
        return defaults.values()

# ----------------- Ø¯Ø§Ù„Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ -----------------

def normalize_text(s):
    """Normalizes input text for simpler matching and processing."""
    if not s:
        return ""
    s = unicodedata.normalize("NFKC", s)
    s = re.sub(r"[\u0610-\u061A\u064B-\u065F\u06D6-\u06ED\u0640]", "", s)
    s = s.lower()
    s = re.sub(r"[^\w\u0600-\u06FF]+", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def simple_stem(word):
    # Simple stemmer for Arabic (as in your original code)
    for p in ("Ø§Ù„", "Ùˆ", "Ù", "Ø¨", "Ù„", "Ùƒ"):
        if word.startswith(p) and len(word) > len(p) + 1:
            word = word[len(p):]
            break
    for s in ("Ù‡", "Ù‡Ø§", "Ø§Ù†", "ÙˆÙ†", "ÙŠÙ†", "Ø§Øª", "Ø©"):
        if word.endswith(s) and len(word) > len(s) + 1:
            word = word[:-len(s)]
            break
    return word

# ----------------- ÙØ¦Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© (ML) -----------------

class AdvancedSentimentAnalyzer:
    def __init__(self, model_name="aubmindlab/bert-base-arabertv2", max_context_turns=5):
        """
        ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø­Ù„Ù„: ÙŠØªÙ… Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø· Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚.
        """
        # Ø¶Ø¨Ø· Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø£ÙˆÙ„Ø§Ù‹
        self.tokenizer = None
        self.model = None
        self.max_context_turns = max_context_turns
        self.labels = {
            0: "Ø­Ø²Ù†/Ø§ÙƒØªØ¦Ø§Ø¨",
            1: "Ù‚Ù„Ù‚/ØªÙˆØªØ±",
            2: "ØªØ¹Ù„Ù‚/Ø­Ø¨ ØªÙ…Ù„Ùƒ",
            3: "Ù‚Ù„Ø© Ø«Ù‚Ø© Ø¨Ø§Ù„Ù†ÙØ³",
            4: "Ù…Ø­Ø§ÙŠØ¯/Ø£Ø®Ø±Ù‰",
        }

        # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙÙ…Ø±Ù‘ÙØ± Ø§Ø³Ù… Ù†Ù…ÙˆØ°Ø¬ Ø£Ùˆ Ù„Ù… ØªØªÙˆÙØ± Ù…ÙƒØªØ¨Ø§Øª transformers/torchØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„ÙØ§Ù„Ø¨Ø§Ùƒ ÙÙ‚Ø·
        if not model_name:
            logger.info("No model_name provided â€” using lexicon fallback only.")
            return

        if not AutoTokenizer or not AutoModelForSequenceClassification:
            logger.warning("transformers or required classes unavailable â€” using lexicon fallback only.")
            return

        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ØªØºÙŠØ± `device` Ø§Ù„Ù…Ø¹Ø±Ù‘Ù Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ÙˆØ­Ø¯Ø© Ù…Ø¨Ø§Ø´Ø±Ø©
        model_device = device 
        logger.info("Loading model %s on %s...", model_name, model_device)
        # ØªØ­Ù…ÙŠÙ„ tokenizer ÙˆØ§Ù„Ù†Ù…ÙˆØ°Ø¬ (Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ ÙˆÙ‚ØªØ§Ù‹ØŒ ÙˆÙ‚Ø¯ ÙŠÙØ´Ù„ Ø¥Ø°Ø§ Ù„Ù… ØªØªÙˆÙØ± Ø§Ù„Ù…ÙˆØ§Ø±Ø¯)
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)
            if TORCH_AVAILABLE and torch is not None and hasattr(self.model, 'to'):
                self.model = self.model.to(model_device)
        except Exception: # Catching a broad exception is acceptable here as many things can go wrong.
            logger.exception("Failed to load ML model; continuing with fallback lexicon approach.")
            self.tokenizer = None
            self.model = None
            return
    def analyze_sentiment(self, user_input, conversation_context):
        """
        ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø·ÙˆÙŠÙ„ (ML).
        ØªØ±Ø¬Ø¹: (label, probabilities_array_or_None, risk_level)
        """
        # Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Ø£Ùˆ Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ØºÙŠØ± Ù…ØªØ§Ø­Ø©)ØŒ Ù†Ø±Ø¬Ø¹ Ø¥Ù„Ù‰ Ø§Ù„Ù€ fallback
        if not getattr(self, 'model', None):
            return self.fallback_sentiment(user_input), None, self.check_for_risk(user_input)

        # 1. ØªØ­Ø¯ÙŠØ« Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
        context_list = [self.preprocess_arabic_text(c) for c in (conversation_context or [])]
        context_list.append(self.preprocess_arabic_text(user_input))

        # 2. ØªÙˆÙ„ÙŠØ¯ Ù…Ù„Ø®Øµ Ø§Ù„Ø³ÙŠØ§Ù‚ (Ù„Ø¢Ø®Ø± N Ø£Ø¯ÙˆØ§Ø±)
        relevant_context = context_list[-self.max_context_turns:]
        contextual_input = " [SEP] ".join(relevant_context)

        # 3. Ø§Ù„ØªØ±Ù…ÙŠØ² ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„
        encoding = self.tokenizer.encode_plus(
            contextual_input,
            max_length=512,
            padding='max_length',
            truncation=True,
            return_tensors='pt',
        )

        # Ø¥Ø°Ø§ ÙƒØ§Ù† torch ØºÙŠØ± Ù…ØªÙˆÙØ±ØŒ Ù†Ø¹ÙŠØ¯ fallback
        if not TORCH_AVAILABLE or torch is None:
            return self.fallback_sentiment(user_input), None, self.check_for_risk(user_input)

        input_ids = encoding['input_ids'].to(device)
        attention_mask = encoding['attention_mask'].to(device)

        # 4. Ø§Ù„ØªÙ†Ø¨Ø¤
        with torch.no_grad():
            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)

        logits = getattr(outputs, 'logits', None)
        if logits is None:
            return self.fallback_sentiment(user_input), None, self.check_for_risk(user_input)

        probabilities = torch.softmax(logits, dim=1).cpu().numpy()[0]
        predicted_class_id = int(np.argmax(probabilities))
        predicted_sentiment = self.labels.get(predicted_class_id, "Ù…Ø­Ø§ÙŠØ¯/Ø£Ø®Ø±Ù‰")

        risk_level = self.check_for_risk(user_input)

        return predicted_sentiment, probabilities, risk_level

    def fallback_sentiment(self, message):
        """
        Lexicon fallback sentiment detection (Ù…Ù†Ø·Ù‚Ù‡ Ø§Ù„Ø£ØµÙ„ÙŠ).
        """
        text = message or ""
        norm = normalize_text(text)
        if not norm:
            return "Ù…Ø­Ø§ÙŠØ¯/Ø£Ø®Ø±Ù‰"

        words_original = norm.split()
        words_stemmed = [simple_stem(w) for w in words_original]
        norm_polarity = {simple_stem(normalize_text(k)): v for k, v in EMOTION_POLARITY.items()}

        score = 0.0
        matches = 0
        for i, w_stem in enumerate(words_stemmed):
            polarity = 0
            if w_stem in norm_polarity:
                polarity = norm_polarity[w_stem]
            # ... (Ù…Ù†Ø·Ù‚ difflib fuzzy matching ÙƒÙ…Ø§ Ù‡Ùˆ) ...

            if polarity != 0:
                matches += 1
                if i > 0 and words_original[i - 1] in ("Ù„Ø§", "Ù„ÙŠØ³", "Ù„Ù…", "Ù„Ù†", "ØºÙŠØ±", "Ù…Ø§"):
                    score -= polarity * 1.5
                else:
                    score += polarity
        
        if matches == 0:
            return "Ù…Ø­Ø§ÙŠØ¯/Ø£Ø®Ø±Ù‰"
        
        avg_polarity = score / max(1, matches)
        
        # Ø¥ØµÙ„Ø§Ø­ Ù…Ù†Ø·Ù‚ Ø§Ù„Ù€ Fallback: Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ù‚Ø±Ø¨ Ø´Ø¹ÙˆØ± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¯Ø±Ø¬Ø© Ø§Ù„Ù‚Ø·Ø¨ÙŠØ© Ø§Ù„Ù…Ø­Ø³ÙˆØ¨Ø©
        # Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø®Ø§Ø·Ø¦ Ù„ÙƒÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø³Ù„Ø¨ÙŠØ© Ø¥Ù„Ù‰ "Ø­Ø²Ù†"
        if not EMOTION_POLARITY:
            return "Ù…Ø­Ø§ÙŠØ¯/Ø£Ø®Ø±Ù‰"

        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ù‚Ø±Ø¨ Ø´Ø¹ÙˆØ± ÙÙŠ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ù„Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù…Ø­Ø³ÙˆØ¨Ø©
        closest_emotion = min(
            EMOTION_POLARITY.keys(),
            key=lambda emotion: abs(EMOTION_POLARITY[emotion] - avg_polarity)
        )
        return closest_emotion

    def check_for_risk(self, text):
        """
        ÙØ­Øµ Ø³Ø±ÙŠØ¹ Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø£Ùˆ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø§Ù„ØªÙŠ ØªØ¯Ù„ Ø¹Ù„Ù‰ Ø­Ø§Ù„Ø© Ø£Ø²Ù…Ø©.
        """
        for word in CRISIS_KEYWORDS:
            if word in text.lower():
                return "Ø®Ø·ÙˆØ±Ø© Ø¹Ø§Ù„ÙŠØ© - ÙŠØ±Ø¬Ù‰ Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©"
        return "Ø®Ø·ÙˆØ±Ø© Ù…Ù†Ø®ÙØ¶Ø©"

    def preprocess_arabic_text(self, text):
        return text.replace('\n', ' ').strip()
        
    def _handle_greeting(self, normalized_text):
        """Handles greeting messages."""
        for key, resp in GREETING_RESPONSES.items():
            if key in normalized_text:
                return resp
        return None

    def _handle_phrase_bank(self, normalized_text):
        """Matches input against the phrase bank."""
        if not (isinstance(PHRASE_BANK, dict) and PHRASE_BANK):
            return None
        try:
            keys = list(PHRASE_BANK.keys())
            # Use a slightly lower cutoff to be more lenient
            match = difflib.get_close_matches(normalized_text, keys, n=1, cutoff=0.45)
            if match:
                return PHRASE_BANK.get(match[0])
        except Exception:
            logger.warning("Error during difflib matching in phrase bank.")
        return None

    def _handle_question(self, user_text):
        """Handles direct questions from the user."""
        if re.search(r'\?|\bÙƒÙŠÙ\b|\bÙ…Ø§Ø°Ø§\b|\bÙ„Ù…Ø§Ø°Ø§\b|\bØ£ÙŠÙ†\b|\bÙ…ØªÙ‰\b|\bÙ‡Ù„\b', user_text):
            snippet = user_text if len(user_text) < 120 else user_text[:117] + '...'
            return (
                f"Ù‚Ø±Ø£Øª Ø³Ø¤Ø§Ù„Ùƒ: Â«{snippet}Â». Ø£Ù‚Ø¯Ù‘Ø± ÙˆØ¶ÙˆØ­Ùƒ ÙÙŠ Ø§Ù„ØªØ¹Ø¨ÙŠØ±. ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø£Ù† Ø£Ù‚Ø¯Ù… Ù„Ùƒ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø¨Ø£Ø­Ø¯ Ø§Ù„Ø£Ø´ÙƒØ§Ù„ Ø§Ù„ØªØ§Ù„ÙŠØ©:\n"
                "- (Ø£) Ø®Ø·ÙˆØ§Øª Ø¹Ù…Ù„ÙŠØ© Ù‚ØµÙŠØ±Ø©.\n"
                "- (Ø¨) ØªÙØ³ÙŠØ± Ù…ÙØµÙ„ Ù„Ù„Ù…ÙˆØ¶ÙˆØ¹.\n"
                "- (Ø¬) Ø£Ù…Ø«Ù„Ø© ÙˆØªØ¬Ø§Ø±Ø¨ Ù…Ø´Ø§Ø¨Ù‡Ø©.\n\n"
                "Ø£ÙŠ Ù†Ù…Ø· ØªÙØ¶Ù„ Ø£Ù† Ø£Ø¨Ø¯Ø£ Ø¨Ù‡ Ø§Ù„Ø¢Ù†ØŸ"
            )
        return None

    def _warm_signature(self):
        return "\n\nØ£Ø¨Ù‚Ù‰ Ù…Ø¹Ùƒ Ù‡Ù†Ø§ØŒ Ù‡Ù„ ØªÙˆØ¯ Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„Ø¢Ù† Ø£Ù… ØªÙØ¶Ù‘Ù„ ØªÙ…Ø±ÙŠÙ†Ù‹Ø§ Ù‚ØµÙŠØ±Ù‹Ø§ØŸ"

    # Ø¯Ø§Ù„Ø© ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯ - ØªÙ… ØªØ¹Ø¯ÙŠÙ„Ù‡Ø§ Ù„ØªÙƒÙˆÙ† Ø¬Ø²Ø¡Ø§Ù‹ Ù…Ù† Ø§Ù„ÙØ¦Ø©
    def generate_response(self, sentiment, risk_level, user_input=None, conversation_context=None):
        """
        ØªÙˆÙ„ÙŠØ¯ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø¹Ø±Ø¨ÙŠØ© ÙØµÙŠØ­Ø©ØŒ Ù…Ø±ØªØ¨Ø©ØŒ ÙˆÙ…ØªØµÙ„Ø© Ø¨Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù….
        Ù…Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø±Ø¯:
        - ÙŠØ¹Ø§Ù„Ø¬ Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø®Ø·Ø± ÙÙˆØ±Ø§Ù‹.
        - ÙŠØ¹ÙƒØ³ Ù…Ø§ Ù‚Ø§Ù„Ù‡ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø­ØªØ±Ù…Ø© ÙˆÙ…ØªÙ…Ø§Ø³ÙƒØ©.
        - ÙŠÙ‚Ø¯Ù… Ø·Ø±ÙˆØ­Ø§Øª Ø¹Ù…Ù„ÙŠØ© ØµØºÙŠØ±Ø© Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„Ø´Ø¨Ø§Ø¨ (ØªÙ‡Ø¯Ø¦Ø©ØŒ ØªÙˆØ§ØµÙ„ØŒ ØªÙ…Ø±ÙŠÙ†Ø§Øª Ù‚ØµÙŠØ±Ø©).
        - ÙŠØ®ØªÙ… Ø¨Ø³Ø¤Ø§Ù„ Ù…ØªØ§Ø¨Ø¹Ø© ÙˆØªØ´Ø¬ÙŠØ¹ ÙˆØ¹Ø¨Ø§Ø±Ø§Øª ÙˆØ¯ÙŠØ©.
        """
        # 1) Handle crisis immediately
        if "Ø®Ø·ÙˆØ±Ø© Ø¹Ø§Ù„ÙŠØ©" in risk_level:
            return CRISIS_RESPONSE_AR

        user_text = (user_input or "").strip()
        norm = normalize_text(user_text)

        # 2) Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø§Øª Ø§Ù„Ù…Ø¨Ù†ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ (Greetings, Phrase Bank, Questions)
        if norm:
            greeting_resp = self._handle_greeting(norm)
            if greeting_resp:
                return f"{greeting_resp} {self._warm_signature()}"

            phrase_bank_resp = self._handle_phrase_bank(norm)
            if phrase_bank_resp:
                return f"{phrase_bank_resp} {self._warm_signature()}"

        if user_text:
            question_resp = self._handle_question(user_text)
            if question_resp:
                return f"{question_resp} {self._warm_signature()}"

        # 5) Ø§Ø³ØªØ¬Ø§Ø¨Ø§Øª Ø¹Ø§Ø·ÙÙŠØ© Ù…ÙÙØµÙ‘ÙÙ„Ø© Ù…Ø¹ Ù…Ù‚ØªØ±Ø­Ø§Øª Ø¹Ù…Ù„ÙŠØ© ØµØºÙŠØ±Ø© Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ·Ø¨ÙŠÙ‚ ÙÙˆØ±Ø§Ù‹
        if isinstance(sentiment, str) and sentiment:
            # Ø§Ù†Ø¹ÙƒØ§Ø³ Ù…ÙˆØ¬Ø² Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„ÙŠØ´Ø¹Ø± Ø¨Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø©
            reflected = f"Ø£Ù†Øª Ù‚Ù„Øª: Â«{user_text}Â»." if user_text else ""

            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¨Ø§Ø´Ø±Ø© Ù„ØªÙ‚Ø¯ÙŠÙ… Ø§Ø³ØªØ¬Ø§Ø¨Ø§Øª Ø£ÙƒØ«Ø± Ø¯Ù‚Ø©
            if "Ø­Ø²Ù†" in sentiment or "ÙˆØ­Ø¯Ø©" in sentiment:
                body = (
                    f"{reflected} Ø£Ø³Ù…Ø¹ Ø«Ù‚Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± ÙÙŠ ÙƒÙ„Ø§Ù…ÙƒØŒ ÙˆÙ‡Ø°Ø§ Ø£Ù…Ø± Ù…ÙÙ‡ÙˆÙ… ØªÙ…Ø§Ù…Ù‹Ø§. Ø§Ù„Ø´Ø¹ÙˆØ± Ø¨Ø§Ù„ÙˆØ­Ø¯Ø© Ù…Ø¤Ù„Ù…ØŒ Ù„ÙƒÙ† ØªØ°ÙƒØ± Ø£Ù†Ùƒ Ù„Ø³Øª ÙˆØ­Ø¯Ùƒ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø´Ø¹ÙˆØ±. "
                    "Ù„Ù‚Ø¯ ØµÙ…Ù…Øª Ù„Ùƒ Ø±Ø­Ù„Ø© ØµØºÙŠØ±Ø© Ø§Ø³Ù…Ù‡Ø§ 'Ø®Ø·ÙˆØ§Øª Ù†Ø­Ùˆ Ø§Ù„ØªÙˆØ§ØµÙ„' Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ. "
                    'Ù‡Ù„ ØªÙˆØ¯ Ø£Ù† ØªØ¨Ø¯Ø£ Ø¨Ø£ÙˆÙ„ Ù…Ù‡Ù…Ø© Ø§Ù„Ø¢Ù†ØŸ Ø¥Ù†Ù‡Ø§ Ø®Ø·ÙˆØ© Ø¨Ø³ÙŠØ·Ø© Ø¬Ø¯Ù‹Ø§ Ù„Ù„Ø¨Ø¯Ø¡.<br>'
                    '<button class="chat-btn" data-input="Ù†Ø¹Ù…ØŒ Ø£Ø±ÙŠØ¯ Ø¨Ø¯Ø¡ Ø£ÙˆÙ„ Ù…Ù‡Ù…Ø© ÙÙŠ Ø±Ø­Ù„Ø© Ø§Ù„ØªÙˆØ§ØµÙ„">ğŸš€ Ù†Ø¹Ù…ØŒ Ù„Ù†Ø¨Ø¯Ø£</button>'
                    '<button class="chat-btn" data-input="Ù„ÙŠØ³ Ø§Ù„Ø¢Ù†ØŒ Ø±Ø¨Ù…Ø§ Ù„Ø§Ø­Ù‚Ø§Ù‹">â±ï¸ Ù„ÙŠØ³ Ø§Ù„Ø¢Ù†</button>'
                )
                return body + self._warm_signature()

            if "Ù‚Ù„Ù‚" in sentiment or "ØªÙˆØªØ±" in sentiment:
                body = (
                    f"{reflected} Ø£Ø±Ù‰ ØªÙˆØªØ±Ù‹Ø§ Ø£Ùˆ Ù‚Ù„Ù‚Ù‹Ø§ ÙˆØ§Ø¶Ø­ÙŠÙ† â€” ÙˆÙ‡Ø°Ø§ Ø´Ø¹ÙˆØ± Ø´Ø§Ø¦Ø¹ ÙˆÙ…Ø¤Ù„Ù…. "
                    "Ù„ØªØ®ÙÙŠÙ Ø§Ù„Ø¶ÙŠÙ‚ ÙÙˆØ±Ù‹Ø§ØŒ Ø£ÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª ØªÙˆØ¯ ØªØ¬Ø±Ø¨ØªÙ‡ Ø§Ù„Ø¢Ù†ØŸ<br>"
                    '<button class="chat-btn" data-input="Ø£Ø±ÙŠØ¯ ØªØ¬Ø±Ø¨Ø© ØªÙ…Ø±ÙŠÙ† Ø§Ù„ØªÙ†ÙØ³ Ø§Ù„Ø¹Ù…ÙŠÙ‚">ğŸ§˜ ØªÙ…Ø±ÙŠÙ† ØªÙ†ÙØ³</button>'
                    '<button class="chat-btn" data-input="Ø£Ø±ÙŠØ¯ ØªØ¯ÙˆÙŠÙ† Ù…Ø§ ÙŠÙ‚Ù„Ù‚Ù†ÙŠ ÙÙŠ Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ø£ÙÙƒØ§Ø±">ğŸ“ ØªØ¯ÙˆÙŠÙ† Ø§Ù„Ø£ÙÙƒØ§Ø±</button>'
                    '<button class="chat-btn" data-input="Ø£Ø±ÙŠØ¯ Ù†ØµÙŠØ­Ø© Ø³Ø±ÙŠØ¹Ø© Ù„Ù„ØªÙ‡Ø¯Ø¦Ø©">ğŸ’¡ Ù†ØµÙŠØ­Ø© Ø³Ø±ÙŠØ¹Ø©</button>'
                )
                return body + self._warm_signature()

            if "ØªØ¹Ù„Ù‚" in sentiment or "Ù‚Ù„Ø© Ø«Ù‚Ø©" in sentiment:
                body = (
                    f"{reflected} Ø£Ø³Ù…Ø¹ ÙÙŠ ÙƒÙ„Ù…Ø§ØªÙƒ Ø£Ù†Ùƒ Ù‚Ø¯ ØªÙƒÙˆÙ† ÙÙŠ ØµØ±Ø§Ø¹ Ù…Ø¹ Ø§Ù„Ø«Ù‚Ø© Ø¨Ø§Ù„Ù†ÙØ³ØŒ ÙˆÙ‡Ø°Ø§ Ø£Ù…Ø± Ù†ÙˆØ§Ø¬Ù‡Ù‡ Ø¬Ù…ÙŠØ¹Ù‹Ø§. "
                    "Ù„Ø¯ÙŠÙ†Ø§ Ø±Ø­Ù„Ø© Ù…ØµÙ…Ù…Ø© Ø®ØµÙŠØµÙ‹Ø§ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø¹Ù„Ù‰ Ø±Ø¤ÙŠØ© Ù‚ÙˆØªÙƒ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© Ø§Ø³Ù…Ù‡Ø§ 'ØªØ¹Ø²ÙŠØ² Ø§Ù„Ø«Ù‚Ø© Ø¨Ø§Ù„Ù†ÙØ³'. "
                    'Ù‡Ù„ ØªÙˆØ¯ Ø£Ù† Ù†Ø¨Ø¯Ø£ Ø¨Ø£ÙˆÙ„ Ø®Ø·ÙˆØ© ØµØºÙŠØ±Ø© Ù…Ø¹Ù‹Ø§ØŸ<br>'
                    '<button class="chat-btn" data-input="Ù†Ø¹Ù…ØŒ Ø£Ø±ÙŠØ¯ Ø¨Ø¯Ø¡ Ø±Ø­Ù„Ø© Ø§Ù„Ø«Ù‚Ø© Ø¨Ø§Ù„Ù†ÙØ³">ğŸ’ª Ù†Ø¹Ù…ØŒ Ù„Ù†Ø¨Ø¯Ø£</button>'
                )
                return body + self._warm_signature()

        # 6) Ø±Ø¯ Ø´Ø§Ù…Ù„ Ø¹Ø§Ù… Ø¹Ù†Ø¯Ù…Ø§ Ù„Ø§ ÙŠÙˆØ¬Ø¯ ØªØ·Ø§Ø¨Ù‚ Ù‚ÙˆÙŠ: ØªØ¹Ø§Ø·ÙØŒ Ø¹Ø±Ø¶ Ø®ÙŠØ§Ø±Ø§Øª Ø¹Ù…Ù„ÙŠØ©ØŒ ÙˆØ³Ø¤Ø§Ù„ Ù…ØªØ§Ø¨Ø¹Ø©
        if user_text:
            snippet = user_text if len(user_text) < 300 else user_text[:297] + '...'
            body = (
                f"Ø´ÙƒØ±Ù‹Ø§ Ù„Ø«Ù‚ØªÙƒ Ø¨Ù…Ø´Ø§Ø±ÙƒØªÙƒ: Â«{snippet}Â». Ù…Ø§ Ø´Ø§Ø±ÙƒØªÙÙ‡ ÙŠÙØ¹Ø·ÙŠ ØµÙˆØ±Ø© Ù…Ù‡Ù…Ø© Ø¹Ù† Ù…Ø§ ØªÙ…Ø±Ù‘ Ø¨Ù‡ØŒ ÙˆØ³Ø£Ø¨Ù‚Ù‰ Ù…Ø¹Ùƒ Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©. "
                "Ø£Ù‚ØªØ±Ø­ Ø®ÙŠØ§Ø±ÙŠÙ†: Ø¥Ù…Ø§ Ø£Ù† Ù†ØªØ§Ø¨Ø¹ Ø¨Ø§Ù„Ø­Ø¯ÙŠØ« Ù„ØªØ¹Ù…ÙŠÙ‚ Ø§Ù„ÙÙ‡Ù…ØŒ Ø£Ùˆ Ø£Ù† Ø£Ø¬Ø±Ø¨ Ù…Ø¹Ùƒ ØªÙ…Ø±ÙŠÙ†Ù‹Ø§ Ù‚ØµÙŠØ±Ù‹Ø§ Ø§Ù„Ø¢Ù† (ØªÙ†ÙÙ‘Ø³/ÙƒØªØ§Ø¨Ø©/ØªÙˆØ§ØµÙ„ ØµØºÙŠØ±). Ø£ÙŠÙ‡Ù…Ø§ ØªÙØ¶Ù‘Ù„ØŸ"
            )
            return body + self._warm_signature()

        # 7)Fallback Ù†Ù‡Ø§Ø¦ÙŠ Ø¨Ø³ÙŠØ· ÙˆØ¯Ø§ÙØ¦
        return (
            "Ø£Ù‡Ù„Ø§Ù‹â€”Ø¥Ù† Ø±ØºØ¨ØªØŒ Ø§Ø¨Ø¯Ø£ Ø¨Ù…Ø´Ø§Ø±ÙƒØ© Ø´ÙŠØ¡ ÙˆØ§Ø­Ø¯ Ø¨Ø³ÙŠØ· Ø¹Ù† ÙŠÙˆÙ…Ùƒ Ø£Ùˆ Ø¹Ù† Ø´Ø¹ÙˆØ±Ùƒ Ø§Ù„Ø¢Ù†ØŒ Ø­ØªÙ‰ Ø¥Ù† ÙƒØ§Ù† Ù…Ø¬Ø±Ø¯ ÙƒÙ„Ù…Ø©. "
            "Ø£Ù†Ø§ Ù‡Ù†Ø§ Ù„Ù„Ø§Ø³ØªÙ…Ø§Ø¹ ÙˆØ§Ù„Ø¯Ø¹Ù… Ø¯ÙˆÙ† Ø£Ø­ÙƒØ§Ù…."
        )

# ----------------- ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù†Ø¯ Ø§Ù„Ø¨Ø¯Ø¡ -----------------

# ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø­Ù„Ù„ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·
# (ÙŠØ¬Ø¨ Ø£Ù† ÙŠØªÙ… Ù‡Ø°Ø§ Ø¨Ø´ÙƒÙ„ Ù…Ø«Ø§Ù„ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… app.before_first_request Ø£Ùˆ ÙÙŠ Ø³ÙŠØ§Ù‚ ØªØ´ØºÙŠÙ„ Flask Ø§Ù„Ù…Ù†Ø§Ø³Ø¨)
try:
    analyzer = AdvancedSentimentAnalyzer()
except Exception as e:
    logger.error(f"Failed to initialize AdvancedSentimentAnalyzer: {e}")
    analyzer = AdvancedSentimentAnalyzer(model_name=None) # ØªÙ‡ÙŠØ¦Ø© Ø¨Ø¯ÙˆÙ† Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ù…ØªØ§Ø¨Ø¹Ø© Ø¨Ø§Ù„Ù€ Fallback

# ----------------- Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª (Routes) -----------------

@app.route('/')
def index():
    # ÙÙŠ ØªØ·Ø¨ÙŠÙ‚ Ø­Ù‚ÙŠÙ‚ÙŠØŒ Ø³ØªØ¹Ø±Ø¶ Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© Ù…Ù„Ù HTML Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ©
    return render_template('index.html')


# Serve original filenames so the HTML can remain unchanged (href="style.css", src="script.js")
@app.route('/style.css')
def serve_root_style():
    return app.send_static_file('style.css')


@app.route('/script.js')
def serve_root_script():
    return app.send_static_file('script.js')

@app.route('/chat', methods=['POST'])
def chat():
    """
    Ù…Ø³Ø§Ø± Ø£Ø³Ø§Ø³ÙŠ Ù„ØªÙ„Ù‚ÙŠ Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙˆÙ…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§.
    """
    try:
        data = request.get_json(force=True)
        user_input = data.get('message', '')
        # Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ£ØªÙŠ Ù…Ù† Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ© (Chat History)
        context = data.get('context', []) 
        
        if not user_input:
            return jsonify({'response': "ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ø±Ø³Ø§Ù„Ø©.", 'sentiment_label': 'Ù…Ø­Ø§ÙŠØ¯'})

        # 1. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± ÙˆØ§Ù„Ù…Ø®Ø§Ø·Ø±
        sentiment, probabilities, risk_level = analyzer.analyze_sentiment(user_input, context)

        # 2. ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© (Ù†Ù…Ø±Ù‘Ø± Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙˆØ§Ù„Ø³ÙŠØ§Ù‚ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø¯ Ù…Ù„Ø§Ø¦Ù…)
        response_text = analyzer.generate_response(sentiment, risk_level, user_input=user_input, conversation_context=context)
        
        # 3. Ø­ÙØ¸ Ø§Ù„ØªÙØ§Ø¹Ù„
        save_interaction(user_input, response_text, sentiment)
        
        return jsonify({
            'response': response_text,
            'sentiment_label': sentiment,
            'probabilities': probabilities.tolist() if probabilities is not None else None,
            'risk_level': risk_level,
            'disclaimer': APP_DISCLAIMER
        })
        
    except Exception as e:
        logger.exception('Failed during chat processing')
        return jsonify({'response': "Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù„Ø§Ø­Ù‚Ø§Ù‹.", 'sentiment_label': 'Ù…Ø­Ø§ÙŠØ¯', 'error': str(e)}), 500

@app.route('/save_interaction', methods=['POST'])
def save_interaction_route():
     # Ø¥Ø¹Ø§Ø¯Ø© ØªÙˆØ¬ÙŠÙ‡ Ù„Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© (Ù„ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¹Ø¨Ø± AJAX)
     try:
         # Using force=False is safer, but requires correct Content-Type header
         payload = request.get_json()
         if not payload:
             return {'status': 'error', 'error': 'Invalid JSON payload'}, 400
         save_interaction(
             payload.get('input', ''),
             payload.get('result', ''),
             payload.get('sentiment_label')
         )
         return {'status': 'ok'}
     except Exception as e:
         return {'status': 'error', 'error': str(e)}, 500

def save_interaction(user_input, response_text, sentiment_label):
    """
    Ø­ÙØ¸ Ø§Ù„ØªÙØ§Ø¹Ù„ ÙÙŠ Ù…Ù„Ù Ø§Ù„Ø³Ø¬Ù„.
    """
    entry = {
        'timestamp': datetime.datetime.utcnow().isoformat() + 'Z',
        'input': user_input,
        'result': response_text,
        'sentiment_label': sentiment_label
    }
    data_dir = os.path.join(os.path.dirname(__file__), 'data')
    os.makedirs(data_dir, exist_ok=True)
    path = os.path.join(data_dir, 'interaction_log.json')
    # Ù…Ù†Ø·Ù‚ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©/Ø§Ù„ÙƒØªØ§Ø¨Ø© ÙƒÙ…Ø§ Ù‡Ùˆ ÙÙŠ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø£ØµÙ„ÙŠ
    try:
         if os.path.exists(path):
            with open(path, 'r', encoding='utf-8') as fh:
                 arr = json.load(fh) if os.path.getsize(path) > 0 else []
         else:
             arr = []
    except (json.JSONDecodeError, Exception):
         arr = []
    
    arr.append(entry)
    with open(path, 'w', encoding='utf-8') as fh:
         json.dump(arr, fh, ensure_ascii=False, indent=2)


# ----------------- ØªÙ…Ø§Ø±ÙŠÙ† ÙŠÙˆÙ…ÙŠØ© ÙˆØªØªØ¨Ø¹ Ø§Ù„ØªÙ‚Ø¯Ù… -----------------
EXERCISES_PATH = os.path.join(os.path.dirname(__file__), 'data', 'exercises.json')
PROGRESS_PATH = os.path.join(os.path.dirname(__file__), 'data', 'progress.json')
USERS_PATH = os.path.join(os.path.dirname(__file__), 'data', 'users.json')

def load_json_file(path, default_value):
    """Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù JSON Ø¨Ø£Ù…Ø§Ù†."""
    if not os.path.exists(path):
        return default_value
    try:
        with open(path, 'r', encoding='utf-8') as fh:
            return json.load(fh)
    except (json.JSONDecodeError, Exception):
        logger.exception('Failed to load or parse JSON file: %s', path)
        return default_value

def load_exercises_file():
    return load_json_file(EXERCISES_PATH, [])

def load_users():
    return load_json_file(USERS_PATH, {})

def save_users(users):
    try:
        os.makedirs(os.path.dirname(USERS_PATH), exist_ok=True)
        with open(USERS_PATH, 'w', encoding='utf-8') as fh:
            json.dump(users, fh, ensure_ascii=False, indent=2)
    except Exception:
        logger.exception('Failed to save users')

def award_badges_for_user(user):
    """Simple badge awarding based on streak length."""
    badges = set(user.get('badges', []))
    streak = int(user.get('streak', 0))
    if streak >= 3:
        badges.add('3-day-streak')
    if streak >= 7:
        badges.add('7-day-streak')
    if streak >= 30:
        badges.add('30-day-streak')
    user['badges'] = sorted(list(badges))
    return user

@app.route('/exercises', methods=['GET'])
def exercises_route():
    ex = load_exercises_file()
    return jsonify({'exercises': ex})


@app.route('/user/create', methods=['POST'])
def create_user():
    try:
        payload = request.get_json(force=True)
        username = (payload.get('username') or '').strip()
        if not username:
            return jsonify({'status': 'error', 'error': 'username required'}), 400
        users = load_users()
        # generate simple id
        user_id = str(uuid.uuid4())
        users[user_id] = {'username': username, 'created': datetime.datetime.utcnow().isoformat() + 'Z'}
        # initialize streak/badges
        users[user_id].update({'streak': 0, 'last_checkin': None, 'badges': []})
        save_users(users)
        return jsonify({'status': 'ok', 'user_id': user_id, 'username': username})
    except Exception as e:
        logger.exception('Failed to create user')
        return jsonify({'status': 'error', 'error': str(e)}), 500


@app.route('/progress', methods=['GET'])
def get_progress():
    user_id = request.args.get('user_id')
    if not user_id:
        return jsonify({'status': 'error', 'error': 'user_id required'}), 400
    try:
        arr = load_json_file(PROGRESS_PATH, [])
        user_records = [r for r in arr if r.get('user_id') == user_id]
        return jsonify({'status': 'ok', 'records': user_records})
    except Exception as e:
        logger.exception('Failed to load exercises')
        return jsonify({'status': 'error', 'error': str(e)}), 500

@app.route('/track_progress', methods=['POST'])
def track_progress():
    try:
        payload = request.get_json(force=True)
        record = {
            'id': str(uuid.uuid4()),
            'timestamp': datetime.datetime.utcnow().isoformat() + 'Z',
            'user_id': payload.get('user_id'),
            'exercise_id': payload.get('exercise_id'),
            'completed': bool(payload.get('completed', False)),
            'note': payload.get('note', '')
        }
        if not record.get('user_id'):
            return jsonify({'status': 'error', 'error': 'user_id required'}), 400
        os.makedirs(os.path.dirname(PROGRESS_PATH), exist_ok=True)
        arr = load_json_file(PROGRESS_PATH, [])
        arr.append(record)
        with open(PROGRESS_PATH, 'w', encoding='utf-8') as fh:
            json.dump(arr, fh, ensure_ascii=False, indent=2)
        return jsonify({'status': 'ok', 'record': record})
    except Exception as e:
        logger.exception('Failed to track progress')
        return jsonify({'status': 'error', 'error': str(e)}), 500

@app.route('/analyze_audio', methods=['POST'])
def analyze_audio():
    # ... (Ø¯Ø§Ù„Ø© analyze_audio ÙƒÙ…Ø§ Ù‡ÙŠ ØªÙ‚Ø±ÙŠØ¨Ø§Ù‹ - ØªÙ… Ø­Ø°ÙÙ‡Ø§ Ù„Ù„Ø§Ø®ØªØµØ§Ø±) ...
    return jsonify({'status': 'error', 'error': 'Audio analysis logic not fully included in this snippet'}), 500


@app.route('/daily_quote', methods=['GET'])
def daily_quote():
    # Return a random motivational quote (can be extended to rotate per-user)
    q = random.choice(MOTIVATIONAL_QUOTES) if MOTIVATIONAL_QUOTES else "Ø£Ù†Øª ØªØ³ØªØ­Ù‚ Ù„Ø­Ø¸Ø© Ù„Ø·Ù Ù…Ø¹ Ù†ÙØ³Ùƒ Ø§Ù„ÙŠÙˆÙ…."
    return jsonify({'quote': q})

@app.route('/resources', methods=['GET'])
def get_resources():
    # Return youth resources loaded from the JSON file
    return jsonify(YOUTH_RESOURCES)

@app.route('/journey', methods=['GET'])
def get_journey_data():
    # Ù…Ø³Ø§Ø± Ø¬Ø¯ÙŠØ¯ Ù„ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø±Ø­Ù„Ø©
    journey_data = load_json_file(os.path.join(os.path.dirname(__file__), 'data', 'journeys.json'), {})
    return jsonify(journey_data)

@app.route('/user/data', methods=['GET'])
def get_user_data():
    user_id = request.args.get('user_id')
    if not user_id:
        return jsonify({'status': 'error', 'error': 'user_id required'}), 400
    
    users = load_users()
    user = users.get(user_id)
    if not user:
        return jsonify({'status': 'error', 'error': 'user not found'}), 404
        
    # ØªØ­Ù…ÙŠÙ„ ØªØ¹Ø±ÙŠÙØ§Øª Ø§Ù„Ø¥Ù†Ø¬Ø§Ø²Ø§Øª Ù„Ø¥Ø±Ø³Ø§Ù„Ù‡Ø§ Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    achievements_def = load_json_file(os.path.join(os.path.dirname(__file__), 'data', 'achievements.json'), {})
    user['achievements_details'] = [achievements_def[ach_id] for ach_id in user.get('achievements', []) if ach_id in achievements_def]

    return jsonify({'status': 'ok', 'user': user})

@app.route('/quest/complete', methods=['POST'])
def complete_quest_route():
    try:
        payload = request.get_json(force=True)
        user_id = payload.get('user_id')
        quest_id = payload.get('quest_id')

        if not user_id or not quest_id:
            return jsonify({'status': 'error', 'error': 'user_id and quest_id are required'}), 400

        users = load_users()
        user = users.get(user_id)
        if not user:
            return jsonify({'status': 'error', 'error': 'user not found'}), 404

        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ù‡Ù…Ø© Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ÙƒØªÙ…Ù„Ø©
        progress = user.get('progress', {})
        if quest_id in progress:
            return jsonify({'status': 'ok', 'message': 'quest already completed', 'user': user})
        
        progress[quest_id] = True
        user['progress'] = progress

        # Ø¥Ø¶Ø§ÙØ© Ù†Ù‚Ø§Ø· Ø§Ù„Ø®Ø¨Ø±Ø©
        journeys = load_json_file(os.path.join(os.path.dirname(__file__), 'data', 'journeys.json'), {})
        quest_xp = 0
        completed_journey_id = None
        for journey_type, journey_data in journeys.items():
            for quest in journey_data.get('quests', []):
                if quest['id'] == quest_id:
                    quest_xp = quest.get('xp', 0)
                    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø±Ø­Ù„Ø© Ù‚Ø¯ Ø§ÙƒØªÙ…Ù„Øª
                    all_quests_in_journey = {q['id'] for q in journey_data.get('quests', [])}
                    if all_quests_in_journey.issubset(set(user['progress'].keys())):
                        completed_journey_id = journey_data.get('id')
                    break
        
        user['xp'] = user.get('xp', 0) + quest_xp

        # Ù…Ù†Ø­ Ø§Ù„Ø¥Ù†Ø¬Ø§Ø² Ø¥Ø°Ø§ Ø§ÙƒØªÙ…Ù„Øª Ø§Ù„Ø±Ø­Ù„Ø©
        new_achievement = None
        if completed_journey_id:
            user_achievements = set(user.get('achievements', []))
            if completed_journey_id not in user_achievements:
                user_achievements.add(completed_journey_id)
                user['achievements'] = sorted(list(user_achievements))
                achievements_def = load_json_file(os.path.join(os.path.dirname(__file__), 'data', 'achievements.json'), {})
                new_achievement = achievements_def.get(completed_journey_id)

        save_users(users)
        return jsonify({'status': 'ok', 'user': user, 'new_achievement': new_achievement})

    except Exception as e:
        logger.exception('Failed to complete quest')
        return jsonify({'status': 'error', 'error': str(e)}), 500

@app.route('/daily_checkin', methods=['POST'])
def daily_checkin():
    try:
        payload = request.get_json(force=True)
        user_id = payload.get('user_id')
        if not user_id:
            return jsonify({'status': 'error', 'error': 'user_id required'}), 400
        users = load_users()
        user = users.get(user_id)
        if not user:
            return jsonify({'status': 'error', 'error': 'user not found'}), 404

        today = datetime.datetime.utcnow().date()
        last = None
        if user.get('last_checkin'):
            try:
                last = datetime.datetime.fromisoformat(user.get('last_checkin')).date()
            except Exception:
                last = None

        if last == today:
            # already checked in today
            return jsonify({'status': 'ok', 'message': 'already_checked_in', 'streak': user.get('streak', 0), 'badges': user.get('badges', [])})

        # increment streak if yesterday was last checkin, otherwise reset to 1
        if last == (today - datetime.timedelta(days=1)):
            user['streak'] = int(user.get('streak', 0)) + 1
        else:
            user['streak'] = 1

        user['last_checkin'] = datetime.datetime.utcnow().isoformat()
        user = award_badges_for_user(user)
        users[user_id] = user
        save_users(users)

        return jsonify({'status': 'ok', 'message': 'checked_in', 'streak': user['streak'], 'badges': user.get('badges', [])})
    except Exception as e:
        logger.exception('daily_checkin failed')
        return jsonify({'status': 'error', 'error': str(e)}), 500


# --- 4. ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ (Execution) ---

if __name__ == "__main__":
    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ© Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„
    default_data = {
        "greetings": GREETING_RESPONSES,
        "emotions": EMOTION_RESPONSES,
        "special": SPECIAL_RESPONSES,
        "examples": EXAMPLE_PHRASES,
        "phrase_bank": PHRASE_BANK,
        "quotes": MOTIVATIONAL_QUOTES,
        "resources": YOUTH_RESOURCES
    }
    GREETING_RESPONSES, EMOTION_RESPONSES, SPECIAL_RESPONSES, EXAMPLE_PHRASES, PHRASE_BANK, MOTIVATIONAL_QUOTES, YOUTH_RESOURCES = load_external_responses(default_data)
    # ÙŠÙØ¶Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù… gunicorn Ø£Ùˆ waitress ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬
    host = '0.0.0.0'
    port = 5000
    print(f"Starting Flask app on http://{host}:{port}/ â€” bind to all interfaces for local testing")
    app.run(host=host, port=port, debug=True)